{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/Users/ckkai/Desktop/ML/project/HOSP_DATA.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['admission_type'].unique()))\n",
    "print(data['admission_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['admission_type'] = data['admission_type'].astype('category')\n",
    "data['admission_type'] = data['admission_type'].cat.codes\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "data['symptoms'] = data['symptoms'].apply(nltk.word_tokenize)\n",
    "data['symptoms']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The result of removal is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word removal\n",
    "# This might change the meaning after work_tokenize.\n",
    "# Do not.....-> ......\n",
    "# In mimiciv database, the symptoms are all important For LOS prediction.\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# data['symptoms'] = data['symptoms'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "# data['symptoms']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming or lemmatization\n",
    "# calculi->calculus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# data['symptoms'] = data['symptoms'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "# data['symptoms']\n",
    "data['symptoms'] = data['symptoms'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "data['symptoms']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY tokenization & punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(\"/Users/ckkai/Desktop/ML/project/HOSP_DATA.csv\")\n",
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "data['admission_type'] = data['admission_type'].astype('category').cat.codes\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = spacy.prefer_gpu()\n",
    "print(gpu)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# define a function to remove punctuation using spacy pipeline\n",
    "def remove_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text for token in doc if not token.is_punct])\n",
    "\n",
    "# apply the remove_punctuation function to the \"symptoms\" column\n",
    "data['symptoms'] = data['symptoms'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_remove_punctuation.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec \n",
    "pre-trained model: https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# After remove.\n",
    "# vectorization=[]\n",
    "# for i in data[\"symptoms\"]:\n",
    "#     bert_none_se_padding=tokenizer.encode(\n",
    "#         text=i,\n",
    "#         add_special_tokens=Ture,\n",
    "#         pad_to_max_length = True,return_attention_mask = True\n",
    "#     )\n",
    "#     vectorization.append(bert_none_se_padding)\n",
    "\n",
    "data=pd.read_csv('data_remove_punctuation.csv')\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for msg in data[\"symptoms\"]:\n",
    "    bert_inp=tokenizer.encode_plus(msg,add_special_tokens = False,max_length =610,padding='max_length',return_attention_mask = True)#,return_tensors='pt'\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338675"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['symptoms']= input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>symptoms</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10467237</td>\n",
       "      <td>20000019</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>25.366667</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10467237</td>\n",
       "      <td>27923538</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>25.366667</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16925328</td>\n",
       "      <td>20000024</td>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>27.069355</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16925328</td>\n",
       "      <td>21837264</td>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>27.069355</td>\n",
       "      <td>1202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16925328</td>\n",
       "      <td>21895449</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>27.069355</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338670</th>\n",
       "      <td>16832788</td>\n",
       "      <td>29998928</td>\n",
       "      <td>6</td>\n",
       "      <td>78</td>\n",
       "      <td>28.485714</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338671</th>\n",
       "      <td>19128791</td>\n",
       "      <td>29998991</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338672</th>\n",
       "      <td>13467099</td>\n",
       "      <td>29999012</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>23.700000</td>\n",
       "      <td>12104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338673</th>\n",
       "      <td>19435486</td>\n",
       "      <td>29999301</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>30.729412</td>\n",
       "      <td>9386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338674</th>\n",
       "      <td>12859020</td>\n",
       "      <td>29999339</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>24.440000</td>\n",
       "      <td>2357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338675 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id   hadm_id  admission_type  age        BMI  symptoms  target\n",
       "0         10467237  20000019               5   76  25.366667       177       0\n",
       "1         10467237  27923538               5   76  25.366667       177       1\n",
       "2         16925328  20000024               5   84  27.069355      1425       0\n",
       "3         16925328  21837264               5   84  27.069355      1202       0\n",
       "4         16925328  21895449               4   84  27.069355       188       0\n",
       "...            ...       ...             ...  ...        ...       ...     ...\n",
       "338670    16832788  29998928               6   78  28.485714       177       1\n",
       "338671    19128791  29998991               4   34  28.000000      6272       0\n",
       "338672    13467099  29999012               5   26  23.700000     12104       0\n",
       "338673    19435486  29999301               8   38  30.729412      9386       0\n",
       "338674    12859020  29999339               2   66  24.440000      2357       0\n",
       "\n",
       "[338675 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert numerical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      " subject_id          int64\n",
      "hadm_id             int64\n",
      "admission_type      int64\n",
      "age                 int64\n",
      "BMI               float64\n",
      "symptoms            int64\n",
      "target              int64\n",
      "dtype: object\n",
      "\n",
      "Final\n",
      " subject_id          int64\n",
      "hadm_id             int64\n",
      "admission_type    float64\n",
      "age               float64\n",
      "BMI               float64\n",
      "symptoms            int64\n",
      "target            float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Original\\n\",data.dtypes)\n",
    "data['admission_type']=data['admission_type'].astype('float')\n",
    "data['age']=data['age'].astype('float')\n",
    "data['target']=data['target'].astype('float')\n",
    "print(\"\\nFinal\\n\",data.dtypes)\n",
    "# data['symptoms']=data['symptoms'].astype('float')\n",
    "type(data['symptoms'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.        , 76.        , 25.36666667],\n",
       "       [ 5.        , 76.        , 25.36666667],\n",
       "       [ 5.        , 84.        , 27.06935484],\n",
       "       ...,\n",
       "       [ 5.        , 26.        , 23.7       ],\n",
       "       [ 8.        , 38.        , 30.72941176],\n",
       "       [ 2.        , 66.        , 24.44      ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['admission_type', 'age', 'BMI']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp={}\n",
    "for i in range(len(input_ids)):\n",
    "    if 610!=len(input_ids[i]):\n",
    "        temp[i]=input_ids[i]\n",
    "        print(len(input_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "# concatenate symptoms with other features\n",
    "X = np.concatenate([data[['admission_type', 'age', 'BMI']].values, np.vstack(input_ids)], axis=1)\n",
    "y = np.array(data[\"target\"])\n",
    "# X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.98, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalization on all input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237072, 613)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8038125711806497\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# C: the smaller , the less over fitting.\n",
    "# kernel: gausian。\n",
    "# gamma: the bigger , the more complex the boundry\n",
    "# degree: improve complex，3 stand for 3 dimension to classify\n",
    "\n",
    "# kernel='linear' model\n",
    "svc_linear=svm.SVC(kernel='linear', C=1)\n",
    "# fit the model\n",
    "svc_linear.fit(X_train, y_train)\n",
    "# prediction\n",
    "predicted1=svc_linear.predict(X_test)\n",
    "# accuracy\n",
    "accuracy1 = svc_linear.score(X_test, y_test)\n",
    "print(predicted1)\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your implementation for finding parameters\n",
    "for c in [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]: #[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "    for sigma in [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]: # gamma=sigma\n",
    "        gamma=np.power(sigma,-2.)/2\n",
    "        model=svm.SVC(C=c,kernel='poly',gamma=gamma)\n",
    "        model.fit(X_train,y_train)\n",
    "        current_score=model.score(X_test,y_test)\n",
    "        if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_params = {'C':c, 'gamma':sigma}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: 0.1, gamma: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "0.8084406956487505\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#  kernel='poly' model\n",
    "svc_poly=svm.SVC(kernel='poly', gamma='auto', C=1)\n",
    "# fit the model\n",
    "svc_poly.fit(X_train[:10000], y_train[:10000])\n",
    "# prediction\n",
    "predicted2=svc_poly.predict(X_test)\n",
    "# accuracy\n",
    "accuracy2 = svc_poly.score(X_test, y_test)\n",
    "print(predicted2)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.7892450181077547\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# C: the smaller , the less over fitting.\n",
    "# kernel: gausian。\n",
    "# gamma: the bigger , the more complex the boundry\n",
    "#  kernel='rbf' model\n",
    "svc_rbf=svm.SVC(kernel='rbf', gamma='0.7', C=1)\n",
    "# fit the model\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "# predition\n",
    "predicted3=svc_rbf.predict(X_test)\n",
    "# accuracy\n",
    "accuracy3 = svc_rbf.score(X_test, y_test)\n",
    "print(predicted3)\n",
    "print(accuracy3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best kernel polynomial kernel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89     80256\n",
      "           1       0.66      0.18      0.28     21347\n",
      "\n",
      "    accuracy                           0.81    101603\n",
      "   macro avg       0.74      0.58      0.59    101603\n",
      "weighted avg       0.78      0.81      0.76    101603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predicted2,labels=[0,1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA downscale text wordtovector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.61391224e+04 -3.46954340e+04  1.04097246e+01 ...  2.17773037e+04\n",
      "   1.50649412e+04 -8.71505628e+03]\n",
      " [ 5.20929911e+04 -9.10120237e+03 -2.83461583e+04 ...  1.67976783e+04\n",
      "   1.34060990e+04  8.66296959e+03]\n",
      " [-2.25731188e+04 -7.32998567e+03  8.49665795e+03 ...  1.08359734e+04\n",
      "   3.43159939e+02 -5.09477423e+03]\n",
      " ...\n",
      " [-4.71125904e+04  2.91516155e+04 -2.09344707e+04 ...  2.38868414e+03\n",
      "   3.73049859e+03  7.90132385e+03]\n",
      " [-1.45063839e+04 -1.25546090e+04  3.89087024e+03 ... -1.04460935e+04\n",
      "   3.54166173e+02 -6.49872088e+03]\n",
      " [-2.49383227e+04 -9.21076627e+03  1.96686684e+04 ...  4.48147040e+03\n",
      "   1.13119758e+03  2.24821294e+03]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=10)\n",
    "input_ids_pca=pca.fit_transform(input_ids) \n",
    "print(input_ids_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = np.concatenate([data[['admission_type', 'age', 'BMI']].values, np.vstack(input_ids_pca)], axis=1)\n",
    "y_pca = np.array(data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y_pca, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.7898290404810882\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# kernel='poly' model\n",
    "svc_poly=svm.SVC(kernel='rbf', degree=3, gamma='auto', C=1)\n",
    "# fit the model\n",
    "svc_poly.fit(X_train_pca, y_train_pca)\n",
    "# predict result\n",
    "predicted4=svc_poly.predict(X_test_pca)\n",
    "# accuracy\n",
    "accuracy4 = svc_poly.score(X_test_pca, y_test_pca)\n",
    "print(predicted4)\n",
    "print(accuracy4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that, whether we change choose the data with pca, normalization or padding with the different size, the result is still around 80%.\n",
    "\n",
    "we try Ensemble learning.(reduce the effect of wordtovector.), if the result is the same, we won't apply the model as base model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembel learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneous base learners (voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "\n",
    "# use decision tree\n",
    "tree = DecisionTreeClassifier(random_state=128)\n",
    "# use SVC\n",
    "svm = SVC(probability=True, random_state=128)\n",
    "# use logistic regression\n",
    "logistic = LogisticRegression(max_iter=10000,random_state=128,penalty='l2')\n",
    "#max_iter 最大迭代次数，逻辑回归虽然叫回归但是并不使用最小二乘法OLS。逻辑回归往往使用坐标法，梯度下降法以及牛顿法，都是需要迭代的方法。因此如果max_iter,会警告说以达到最大迭代次数，但是还是没有收敛的情况。\n",
    "# use KNN\n",
    "Knn = KNeighborsClassifier(n_neighbors=4)\n",
    "# use GaussianNB\n",
    "Gsn = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ensemble using the VotingClassifier\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('tree', tree), ('svm', svm), ('logistic', logistic),('KNN', Gsn),('GaussianNB', Knn)],\n",
    "    voting='hard'  # Use hard voting for majority rule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# concatenate symptoms with other features\n",
    "X_ensemble_all = np.concatenate([data[['admission_type', 'age', 'BMI']].values, np.vstack(input_ids)], axis=1)[:12000]\n",
    "X_ensemble_nemeric = data[['admission_type', 'age', 'BMI']].values[:12000]\n",
    "X_ensemble_wtv = input_ids[:12000]\n",
    "y = np.array(data[\"target\"])[:12000]\n",
    "\n",
    "#scale the data.\n",
    "X_ensemble_all=scaler.fit_transform(X_ensemble_all)\n",
    "\n",
    "# Here use KFold, don't split the dataset handly.\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ensemble_all, X_test_ensemble_all, y_train_ensemble_all, y_test_ensemble_all = train_test_split(X_ensemble_all, y, test_size=0.3, random_state=42)\n",
    "X_train_ensemble_nemeric, X_test_ensemble_nemeric, y_train_ensemble_nemeric, y_test_ensemble_nemeric = train_test_split(X_ensemble_nemeric, y, test_size=0.3, random_state=42)\n",
    "X_train_ensemble_wtv, X_test_ensemble_wtv, y_train_ensemble_wtv, y_test_ensemble_wtv = train_test_split(X_ensemble_wtv, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Ensemble (Majority Voting):\n",
      "Ensemble: 0.799\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold=KFold(n_splits=10)\n",
    "ensemble.fit(X_train_ensemble_all,y_train_ensemble_all)\n",
    "ensemble_scores=cross_val_score(ensemble, X_ensemble_all, y, cv=kfold,n_jobs=-1)\n",
    "# Print the mean accuracy of the ensemble classifier\n",
    "print(\"Mean Accuracy of Ensemble (Majority Voting):\")\n",
    "print(f\"Ensemble: {ensemble_scores.mean():.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The result still stay around 80%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitation we supposed is the dimension of text features is too large which causes bias in the distribution of the whole data or the model is not complex enough to fit all the features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
